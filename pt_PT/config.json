{
  "noInstanceSelected": "Nenhum modelo selecionado",
  "resetToDefault": "Reiniciar",
  "showAdvancedSettings": "Mostrar configurações avançadas",
  "showAll": "Mostrar tudo",
  "basicSettings": "Básico",
  "configSubtitle": "Carregar ou gravar presets e experimentar com modificações de parâmetros do modelo",
  "inferenceParameters/title": "Parâmetros de previsão",
  "inferenceParameters/info": "Experimente com parâmetros que impactam a previsão.",
  "generalParameters/title": "Geral",
  "samplingParameters/title": "Amostragem",
  "basicTab": "Básico",
  "advancedTab": "Avançado",
  "advancedTab/title": "Configuração avançada",
  "advancedTab/expandAll": "Expandir tudo",
  "advancedTab/overridesTitle": "Modificações de configuração",
  "advancedTab/noConfigsText": "Não há alterações por gravar - edite os valores acima para ver as modificações aqui.",
  "loadInstanceFirst": "Carregar um modelo para visualizar parâmetros configuráveis",
  "noListedConfigs": "Não há parâmetros configuráveis",
  "generationParameters/info": "Experimente com parâmetros básicos que impactam a geração de texto.",
  "loadParameters/title": "Carregar Parâmetros",
  "loadParameters/description": "Configurações para controlar a forma como o modelo é inicializado e carregado na memória.",
  "loadParameters/reload": "Recarregar para aplicar alterações",
  "discardChanges": "Cancelar alterações",
  "llm.prediction.systemPrompt/title": "Prompt do Sistema",
  "llm.prediction.systemPrompt/description": "Use este campo para fornecer instruções de fundo ao modelo, como um conjunto de regras, restrições ou requisitos gerais. Este campo é também frequentemente referido como o \"prompt do sistema\".",
  "llm.prediction.systemPrompt/subTitle": "Diretrizes para a IA",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/subTitle": "How much randomness to introduce. 0 will yield the same result every time, while higher values will increase creativity and variance",
  "llm.prediction.temperature/info": "De acordo com os documentos de ajuda de llama.cpp: \"O valor predefinido é <{{dynamicValue}}>, que fornece um equilíbrio entre aleatoriedade e determinismo. Uma temperatura de 0 escolherá sempre o token mais provável, levando a saídas idênticas em cada execução\"",
  "llm.prediction.llama.sampling/title": "Sampling",
  "llm.prediction.llama.topKSampling/title": "Amostragem Top K",
  "llm.prediction.llama.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.llama.topKSampling/info": "De acordo com os documentos de ajuda de llama.cpp:\n\nA amostragem top-k é um método de geração de texto que seleciona o próximo token apenas dos k tokens mais prováveis previstos pelo modelo.\n\nIsto ajuda a reduzir o risco de gerar tokens de baixa probabilidade ou sem sentido, mas pode também limitar a diversidade do texto gerado.\n\nUm valor maior para top-k (por exemplo, 100) considerará mais tokens e levará a textos mais diversos, enquanto um valor menor (por exemplo, 10) concentrar-se-á nos tokens mais prováveis e gerará textos mais previsíveis.\n\n• O valor predefinido é <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Núcleos de CPU",
  "llm.prediction.llama.cpuThreads/subTitle": "Number of CPU threads to use during inference",
  "llm.prediction.llama.cpuThreads/info": "O número de núcleos a usar durante a computação. Aumentar o número de núcleos não sempre correlaciona com melhor desempenho. O valor predefinido é <{{dynamicValue}}>",
  "llm.prediction.maxPredictedTokens/title": "Limitar tamanho da resposta",
  "llm.prediction.maxPredictedTokens/subTitle": "Opcionalmente, limite o comprimento da resposta da IA",
  "llm.prediction.maxPredictedTokens/info": "Controle o tamanho máximo da resposta do chatbot. Ative para definir um limite no tamanho máximo de uma resposta ou desative para deixar que o chatbot decida quando parar.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Comprimento máximo da resposta (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Por volta de {{maxWords}} palavras",
  "llm.prediction.llama.repeatPenalty/title": "Penalização por repetição",
  "llm.prediction.llama.repeatPenalty/subTitle": "How much to discourage repeating the same token",
  "llm.prediction.llama.repeatPenalty/info": "De acordo com os documentos de ajuda de llama.cpp: \"Ajuda a prevenir o modelo de gerar texto repetitivo ou monótono.\n\nUm valor maior (por exemplo, 1,5) penalizará as repetições de forma mais vincada, enquanto um valor menor (por exemplo, 0,9) será mais permissivo.\" • O valor predefinido é <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "Amostragem Min P",
  "llm.prediction.llama.minPSampling/subTitle": "Minimum base probability for a token to be selected for output",
  "llm.prediction.llama.minPSampling/info": "De acordo com os documentos de ajuda de llama.cpp:\n\nA probabilidade mínima para um token ser considerado em relação à probabilidade do token mais provável. Deve estar em [0, 1].\n\n• O valor predefinido é <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Amostragem Top P",
  "llm.prediction.llama.topPSampling/subTitle": "Minimum cumulative probability for the possible next tokens. Acts similarly to temperature",
  "llm.prediction.llama.topPSampling/info": "De acordo com os documentos de ajuda de llama.cpp:\n\nA amostragem top-p, também conhecida como amostragem de núcleo, é outro método de geração de texto que seleciona o próximo token apenas de um conjunto de tokens cujas probabilidades acumuladas são pelo menos p.\n\nEste método fornece um equilíbrio entre diversidade e qualidade considerando tanto as probabilidades dos tokens quanto o tamanho de tokens para amostrar.\n\nUm valor mais elevado para top-p (por exemplo, 0,95) levará a textos mais diversos, enquanto um valor menor (por exemplo, 0,5) gerará textos mais focados e previsíveis. Deve estar em (0, 1].\n\n• O valor predefinido é <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Strings de paragem",
  "llm.prediction.stopStrings/subTitle": "Strings que devem parar o modelo de gerar mais tokens",
  "llm.prediction.stopStrings/info": "Strings específicas que, quando encontradas, farão com que o modelo pare de gerar mais tokens",
  "llm.prediction.stopStrings/placeholder": "Insira uma string e pressione ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Interrupção de chat",
  "llm.prediction.contextOverflowPolicy/subTitle": "How the model should behave when the conversation grows too large for it to handle",
  "llm.prediction.contextOverflowPolicy/info": "Decide o que fazer quando o chat excede o tamanho da memória do modelo ('contexto')",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Parar no limite",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Para de gerar uma vez que a memória do modelo se encha",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Truncar meio",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Remove mensagens do meio do chat para dar espaço para novas. O modelo ainda lembrará o início da chat",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Mais recente",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "O modelo obterá sempre as últimas mensagens, mas pode esquecer o início da chat",
  "llm.prediction.llama.frequencyPenalty/title": "Penalização por frequência",
  "llm.prediction.llama.presencePenalty/title": "Penalização por presença",
  "llm.prediction.llama.tailFreeSampling/title": "Amostragem Tail-Free",
  "llm.prediction.llama.locallyTypicalSampling/title": "Amostragem localmente típica",
  "llm.prediction.mlx.repeatPenalty/title": "Penalização por repetição",
  "llm.prediction.mlx.repeatPenalty/subTitle": "How much to discourage repeating the same token",
  "llm.prediction.mlx.repeatPenalty/info": "Um valor maior desencoraja que o modelo se repita",
  "llm.prediction.onnx.topKSampling/title": "Amostragem Top K",
  "llm.prediction.onnx.topKSampling/subTitle": "Limits the next token to one of the top-k most probable tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topKSampling/info": "De acordo com a documentação ONNX:\n\nNúmero de tokens de vocabulário mais prováveis para manter para top-k-filtering\n\n• Este filtro está desligado por princípio",
  "llm.prediction.onnx.repeatPenalty/title": "Penalização por repetição",
  "llm.prediction.onnx.repeatPenalty/subTitle": "How much to discourage repeating the same token",
  "llm.prediction.onnx.repeatPenalty/info": "Um valor maior desencoraja que o modelo se repita",
  "llm.prediction.onnx.topPSampling/title": "Amostragem Top P",
  "llm.prediction.onnx.topPSampling/subTitle": "Minimum cumulative probability for the possible next tokens. Acts similarly to temperature",
  "llm.prediction.onnx.topPSampling/info": "De acordo com a documentação ONNX:\n\nSó os tokens mais prováveis com probabilidades que somam até TopP ou mais são mantidos para geração\n\n• Este filtro está desligado por princípio",
  "llm.prediction.seed/title": "Semente",
  "llm.prediction.structured/title": "Geração estruturada",
  "llm.prediction.structured/info": "Geração estruturada",
  "llm.prediction.promptTemplate/title": "Modelo de prompt",
  "llm.prediction.promptTemplate/subTitle": "The format in which messages in chat are sent to the model. Changing this may introduce unexpected behavior - make sure you know what you're doing!",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "Ocorreu um erro ao processar modelo Jinja: {{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "Manual",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "Antes do sistema",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "Insira prefixo do sistema...",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "Depois do sistema",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "Insira sufixo do sistema...",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "Antes do utilizador",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": "Insira prefixo do utilizador...",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "Depois do utilizador",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "Insira sufixo do utilizador...",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "Antes do assistente",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "Insira prefixo do assistente...",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "Depois do assistente",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": "Insira sufixo do assistente...",
  "llm.prediction.promptTemplate.stopStrings/label": "Strings de paragem adicionais",
  "llm.prediction.promptTemplate.stopStrings/subTitle": "Strings de paragem específicas do modelo que serão usadas além das strings de paragem especificadas pelo utilizador",
  
  "llm.load.contextLength/title": "Tamanho do contexto",
  "llm.load.contextLength/subTitle": "The maximum number of tokens the model can attend to in one prompt. See the Conversation Overflow options under \"Inference params\" for more ways to manage this",
  "llm.load.contextLength/info": "Especifica o número máximo de tokens que o modelo pode considerar ao mesmo tempo, afectando o tamanho do contexto retido durante o processamento",
  "llm.load.seed/title": "Semente",
  "llm.load.seed/subTitle": "The seed for the random number generator used in text generation. -1 is random",
  "llm.load.seed/info": "Semente aleatória: Define a semente para geração de números aleatórios para garantir resultados reprodutíveis",
  
  "llm.load.llama.evalBatchSize/title": "Tamanho do lote de avaliação",
  "llm.load.llama.evalBatchSize/subTitle": "Number of input tokens to process at a time. Increasing this increases performance at the cost of memory usage",
  "llm.load.llama.evalBatchSize/info": "Define o número de exemplos processados em conjunto num lote durante a avaliação, afetando velocidade e uso de memória",
  "llm.load.llama.ropeFrequencyBase/title": "Frequência base da Codificação Posicional Rotativa (RoPE)",
  "llm.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "llm.load.llama.ropeFrequencyBase/info": "[Avançado] Ajusta a frequência base para Codificação Posicional Rotativa (RoPE), afetando como informações posicionais são incorporadas",
  "llm.load.llama.ropeFrequencyScale/title": "Escala de Frequência da Codificação Posicional Rotativa (RoPE)",
  "llm.load.llama.ropeFrequencyScale/subTitle": "Context length is scaled by this factor to extend effective context using RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Avançado] Modifica a escala da frequência para Codificação Posicional Rotativa (RoPE) para controlar a granularidade da codificação posicionai",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "Number of discrete model layers to compute on the GPU for GPU acceleration",
  "llm.load.llama.acceleration.offloadRatio/info": "Set the number of layers to offload to the GPU.",
  "llm.load.llama.flashAttention/title": "Atenção Flash",
  "llm.load.llama.flashAttention/subTitle": "Decreases memory usage and generation time on some models",
  "llm.load.llama.flashAttention/info": "Acelera mecanismos de atenção para um processamento mais rápido e eficiente",
  "llm.load.llama.keepModelInMemory/title": "Manter modelo em memória",
  "llm.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "llm.load.llama.keepModelInMemory/info": "Previne que o modelo seja alocado para o disco, garantindo acesso mais rápido mas com a desvantagem de usar mais memória RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Usar FP16 para Cache KV",
  "llm.load.llama.useFp16ForKVCache/info": "Reduz o uso de memória utilizando a cache com metade da precisão (FP16)",
  "llm.load.llama.tryMmap/title": "Tentar mmap()",
  "llm.load.llama.tryMmap/subTitle": "Improves load time for the model. Disabling this may improve performance when the model is larger than the available system RAM",
  "llm.load.llama.tryMmap/info": "Carrega ficheiros do modelo diretamente do disco para a memória",

  "embedding.load.contextLength/title": "Tamanho do contexto",
  "embedding.load.contextLength/subTitle": "The maximum number of tokens the model can attend to in one prompt. See the Conversation Overflow options under \"Inference params\" for more ways to manage this",
  "embedding.load.contextLength/info": "Especifica o número máximo de tokens que o modelo pode considerar em simultâneo, afectando o tamanho do contexto retido durante o processamento",
  "embedding.load.llama.ropeFrequencyBase/title": "Frequência Base da Codificação Posicional Rotativa (RoPE)",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "Custom base frequency for rotary positional embeddings (RoPE). Increasing this may enable better performance at high context lengths",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avançado] Ajusta a frequência base para Codificação Posicional Rotativa, afetando como informações posicionais são incorporadas",
  "embedding.load.llama.evalBatchSize/title": "Tamanho da Lote de Avaliação",
  "embedding.load.llama.evalBatchSize/subTitle": "Number of input tokens to process at a time. Increasing this increases performance at the cost of memory usage",
  "embedding.load.llama.evalBatchSize/info": "Define o número de tokens processados juntos em um lote durante a avaliação",
  "embedding.load.llama.ropeFrequencyScale/title": "Escala de Frequência da Codificação Posicional Rotativa (RoPE)",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "Context length is scaled by this factor to extend effective context using RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avançado] Modifica a escala da frequência para Codificação Posicional Rotativa (RoPE) para controlar a granularidade da codificação posicionai",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU Offload",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "Number of discrete model layers to compute on the GPU for GPU acceleration",
  "embedding.load.llama.acceleration.offloadRatio/info": "Set the number of layers to offload to the GPU.",
  "embedding.load.llama.keepModelInMemory/title": "Manter modelo em memória",
  "embedding.load.llama.keepModelInMemory/subTitle": "Reserve system memory for the model, even when offloaded to GPU. Improves performance but requires more system RAM",
  "embedding.load.llama.keepModelInMemory/info": "Previne que o modelo seja alocado para o disco, garantindo acesso mais rápido mas com a desvantagem de usar mais memória RAM",
  "embedding.load.llama.tryMmap/title": "Tentar mmap()",
  "embedding.load.llama.tryMmap/subTitle": "Improves load time for the model. Disabling this may improve performance when the model is larger than the available system RAM",
  "embedding.load.llama.tryMmap/info": "Carrega os ficheiros do modelo diretamente do disco para a memória",
  "embedding.load.seed/title": "Semente",
  "embedding.load.seed/subTitle": "The seed for the random number generator used in text generation. -1 is random seed",
  "embedding.load.seed/info": "Semente aleatória: Define a semente para geração de números aleatórios para garantir resultados reproduzíveis"
  }
