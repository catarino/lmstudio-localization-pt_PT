{
  "noInstanceSelected": "Nenhum modelo selecionado",
  "resetToDefault": "Resetar",
  "showAdvancedSettings": "Mostrar configurações avançadas",
  "showAll": "Mostrar tudo",
  "basicSettings": "Básico",
  "configSubtitle": "Carregar ou salvar presets e experimentar com sobrescritas de parâmetros do modelo",
  "inferenceParameters/title": "Parâmetros de previsão",
  "inferenceParameters/info": "Experimente com parâmetros que impactam a previsão.",
  "generalParameters/title": "Geral",
  "samplingParameters/title": "Amostragem",
  "basicTab": "Básico",
  "advancedTab": "Avançado",
  "advancedTab/title": "Configuração avançada",
  "advancedTab/expandAll": "Expandir tudo",
  "advancedTab/overridesTitle": "Sobrescritas de configuração",
  "advancedTab/noConfigsText": "Não há alterações não salvas - edite os valores acima para ver as sobrescritas aqui.",
  "loadInstanceFirst": "Carregar um modelo para visualizar parâmetros configuráveis",
  "noListedConfigs": "Nenhum parâmetro configurável listado",
  "generationParameters/info": "Experimente com parâmetros básicos que impactam a geração de texto.",
  "loadParameters/title": "Carregar Parâmetros",
  "loadParameters/description": "Configurações para controlar a forma como o modelo é inicializado e carregado na memória.",
  "loadParameters/reload": "Recarregar para aplicar alterações",
  "discardChanges": "Descartar alterações",

  "llm.prediction.systemPrompt/title": "Prompt do Sistema",
  "llm.prediction.systemPrompt/description": "Use este campo para fornecer instruções de fundo ao modelo, como um conjunto de regras, restrições ou requisitos gerais. Este campo é também frequentemente referido como o "prompt do sistema".",
  "llm.prediction.systemPrompt/subTitle": "Diretrizes para a Inteligência Artificial",
  "llm.prediction.temperature/title": "Temperatura",
  "llm.prediction.temperature/info": "De acordo com os documentos de ajuda de llama.cpp: "O valor padrão é <{{dynamicValue}}>, que fornece um equilíbrio entre aleatoriedade e determinismo. No extremo, uma temperatura de 0 sempre escolherá o token mais provável, levando a saídas idênticas em cada execução"",
  "llm.prediction.llama.topKSampling/title": "Amostragem Top K",
  "llm.prediction.llama.topKSampling/info": "De acordo com os documentos de ajuda de llama.cpp:\n\nA amostragem top-k é um método de geração de texto que seleciona o próximo token apenas dos k tokens mais prováveis previstos pelo modelo.\n\nIsso ajuda a reduzir o risco de gerar tokens de baixa probabilidade ou sem sentido, mas pode também limitar a diversidade da saída.\n\nUm valor maior para top-k (por exemplo, 100) considerará mais tokens e levará a textos mais diversos, enquanto um valor menor (por exemplo, 10) se concentrará nos tokens mais prováveis e gerará textos mais conservadores.\n\n• O valor padrão é <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "Núcleos de CPU",
  "llm.prediction.llama.cpuThreads/info": "O número de threads a usar durante a computação. Aumentar o número de threads não sempre correlaciona com melhor desempenho. O valor padrão é <{{dynamicValue}}>",
  "llm.prediction.maxPredictedTokens/title": "Limitar Tamanho da Saída",
  "llm.prediction.maxPredictedTokens/subTitle": "Opcionalmente, limite o comprimento do resposta do AI",
  "llm.prediction.maxPredictedTokens/info": "Controle o tamanho máximo da resposta do chatbot. Ative para definir um limite no tamanho máximo de uma resposta ou desative para deixar que o chatbot decida quando parar.",
  "llm.prediction.maxPredictedTokens/inputLabel": "Comprimento máximo da resposta (tokens)",
  "llm.prediction.maxPredictedTokens/wordEstimate": "Sobre {{maxWords}} palavras",
  "llm.prediction.llama.repeatPenalty/title": "Multa por Repetição",
  "llm.prediction.llama.repeatPenalty/info": "De acordo com os documentos de ajuda de llama.cpp: "Ajuda a prevenir o modelo de gerar texto repetitivo ou monótono.\n\nUm valor maior (por exemplo, 1,5) penalizará as repetições mais fortemente, enquanto um valor menor (por exemplo, 0,9) será mais leniente." • O valor padrão é <{{dynamicValue}}>",
  "llm.prediction.llama.minPSampling/title": "Amostragem Min P",
  "llm.prediction.llama.minPSampling/info": "De acordo com os documentos de ajuda de llama.cpp:\n\nA probabilidade mínima para um token ser considerado, em relação à probabilidade do token mais provável. Deve estar em [0, 1].\n\n• O valor padrão é <{{dynamicValue}}>",
  "llm.prediction.llama.topPSampling/title": "Amostragem Top P",
  "llm.prediction.llama.topPSampling/info": "De acordo com os documentos de ajuda de llama.cpp:\n\nA amostragem top-p, também conhecida como amostragem de núcleo, é outro método de geração de texto que seleciona o próximo token apenas de um conjunto de tokens cujas probabilidades acumuladas são pelo menos p.\n\nEste método fornece um equilíbrio entre diversidade e qualidade considerando tanto as probabilidades dos tokens quanto a quantidade de tokens para amostrar.\n\nUm valor maior para top-p (por exemplo, 0,95) levará a textos mais diversos, enquanto um valor menor (por exemplo, 0,5) gerará textos mais focados e conservadores. Deve estar em (0, 1].\n\n• O valor padrão é <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "Strings de Parada",
  "llm.prediction.stopStrings/subTitle": "Strings que devem parar o modelo de gerar mais tokens",
  "llm.prediction.stopStrings/info": "Strings específicas que, quando encontradas, farão com que o modelo pare de gerar mais tokens",
  "llm.prediction.stopStrings/placeholder": "Insira uma string e pressione ⏎",
  "llm.prediction.contextOverflowPolicy/title": "Política de Sobrecarga de Conversa",
  "llm.prediction.contextOverflowPolicy/info": "Decida o que fazer quando a conversa excede o tamanho da memória de trabalho do modelo ('contexto')",
  "llm.prediction.contextOverflowPolicy/stopAtLimit": "Parar no Limite",
  "llm.prediction.contextOverflowPolicy/stopAtLimitSub": "Pare de gerar uma vez que a memória do modelo se encha",
  "llm.prediction.contextOverflowPolicy/truncateMiddle": "Truncar Meio",
  "llm.prediction.contextOverflowPolicy/truncateMiddleSub": "Remove mensagens do meio da conversa para fazer espaço para novas. O modelo ainda lembrará o início da conversa",
  "llm.prediction.contextOverflowPolicy/rollingWindow": "Janela Rodante",
  "llm.prediction.contextOverflowPolicy/rollingWindowSub": "O modelo sempre obterá as últimas mensagens, mas pode esquecer o início da conversa",
  "llm.prediction.llama.frequencyPenalty/title": "Multas por Frequência",
  "llm.prediction.llama.presencePenalty/title": "Multas por Presença",
  "llm.prediction.llama.tailFreeSampling/title": "Amostragem sem Cabeça",
  "llm.prediction.llama.locallyTypicalSampling/title": "Amostragem Localmente Típica",
  "llm.prediction.mlx.repeatPenalty/title": "Multas por Repetição",
  "llm.prediction.mlx.repeatPenalty/info": "Um valor maior desencoraja o modelo de se repetir",
  "llm.prediction.onnx.topKSampling/title": "Amostragem Top K",
  "llm.prediction.onnx.topKSampling/info": "De acordo com a documentação ONNX:\n\nNúmero de tokens de vocabulário mais prováveis para manter para top-k-filtering\n\n• Este filtro está desligado por padrão",
  "llm.prediction.onnx.repeatPenalty/title": "Multas por Repetição",
  "llm.prediction.onnx.repeatPenalty/info": "Um valor maior desencoraja o modelo de se repetir",
  "llm.prediction.onnx.topPSampling/title": "Amostragem Top P",
  "llm.prediction.onnx.topPSampling/info": "De acordo com a documentação ONNX:\n\nSó os tokens mais prováveis com probabilidades que somam até TopP ou mais são mantidos para geração\n\n• Este filtro está desligado por padrão",
  "llm.prediction.seed/title": "Semente",
  "llm.prediction.structured/title": "Saída Estruturada",
  "llm.prediction.structured/info": "Saída Estruturada",
  "llm.prediction.promptTemplate/title": "Modelo de Prompt",
  "llm.prediction.promptTemplate.types.jinja/label": "Jinja",
  "llm.prediction.promptTemplate.types.jinja/error": "Falha ao parsear modelo Jinja: {{error}}",
  "llm.prediction.promptTemplate.types.manual/label": "Manual",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/label": "Antes do Sistema",
  "llm.prediction.promptTemplate.manual.subfield.beforeSystem/placeholder": "Insira prefixo de sistema...",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/label": "Depois do Sistema",
  "llm.prediction.promptTemplate.manual.subfield.afterSystem/placeholder": "Insira sufixo de sistema...",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/label": "Antes do Usuário",
  "llm.prediction.promptTemplate.manual.subfield.beforeUser/placeholder": "Insira prefixo de usuário...",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/label": "Depois do Usuário",
  "llm.prediction.promptTemplate.manual.subfield.afterUser/placeholder": "Insira sufixo de usuário...",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/label": "Antes da Assistente",
  "llm.prediction.promptTemplate.manual.subfield.beforeAssistant/placeholder": "Insira prefixo de assistente...",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/label": "Depois da Assistente",
  "llm.prediction.promptTemplate.manual.subfield.afterAssistant/placeholder": "Insira sufixo de assistente...",
  "llm.prediction.promptTemplate.stopStrings/label": "Strings de Parada Adicionais",
  "llm.prediction.promptTemplate.stopStrings/hint": "Strings de parada específicas do modelo que serão usadas além das strings de parada especificadas pelo usuário",

  "llm.load.contextLength/title": "Comprimento do Contexto",
  "llm.load.contextLength/info": "Especifica o número máximo de tokens que o modelo pode considerar ao mesmo tempo, impactando a quantidade de contexto retido durante o processamento",
  "llm.load.seed/title": "Semente",
  "llm.load.seed/info": "Semente Aleatória: Define a semente para geração de números aleatórios para garantir resultados reprodutíveis",

  "llm.load.llama.evalBatchSize/title": "Tamanho da Lote de Avaliação",
  "llm.load.llama.evalBatchSize/info": "Define o número de exemplos processados juntos em um lote durante a avaliação, afetando velocidade e uso de memória",
  "llm.load.llama.ropeFrequencyBase/title": "Frequência Base do RoPE",
  "llm.load.llama.ropeFrequencyBase/info": "[Avançado] Ajusta a frequência base para Codificação Posicional Rotativa, afetando como informações posicionais são incorporadas",
  "llm.load.llama.ropeFrequencyScale/title": "Escala de Frequência do RoPE",
  "llm.load.llama.ropeFrequencyScale/info": "[Avançado] Modifica a escala da frequência para Codificação Posicional Rotativa para controlar a granularidade da codificação posicionai",
  "llm.load.llama.gpuOffload/title": "Desligar GPU",
  "llm.load.llama.gpuOffload/info": "Defina a razão de cálculo para deslocar para o GPU. Definir como off desativa a deslocação do GPU, ou auto deixe que o modelo decida.",
  "llm.load.llama.flashAttention/title": "Atenção Flash",
  "llm.load.llama.flashAttention/info": "Acelera mecanismos de atenção para processamento mais rápido e eficiente",
  "llm.load.llama.keepModelInMemory/title": "Manter Modelo na Memória",
  "llm.load.llama.keepModelInMemory/info": "Prevenir que o modelo seja intercambiado com a unidade de disco, garantindo acesso mais rápido ao custo de uso maior da memória RAM",
  "llm.load.llama.useFp16ForKVCache/title": "Usar FP16 para Cache KV",
  "llm.load.llama.useFp16ForKVCache/info": "Reduz o uso de memória reduzindo a cache em metade-precisão (FP16)",
  "llm.load.llama.tryMmap/title": "Tentar mmap()",
  "llm.load.llama.tryMmap/info": "Carregar arquivos do modelo diretamente da unidade de disco para a memória",

  "embedding.load.contextLength/title": "Comprimento do Contexto",
  "embedding.load.contextLength/info": "Especifica o número máximo de tokens que o modelo pode considerar ao mesmo tempo, impactando a quantidade de contexto retido durante o processamento",
  "embedding.load.llama.ropeFrequencyBase/title": "Frequência Base do RoPE",
  "embedding.load.llama.ropeFrequencyBase/info": "[Avançado] Ajusta a frequência base para Codificação Posicional Rotativa, afetando como informações posicionais são incorporadas",
  "embedding.load.llama.evalBatchSize/title": "Tamanho da Lote de Avaliação",
  "embedding.load.llama.evalBatchSize/info": "Define o número de tokens processados juntos em um lote durante a avaliação",
  "embedding.load.llama.ropeFrequencyScale/title": "Escala de Frequência do RoPE",
  "embedding.load.llama.ropeFrequencyScale/info": "[Avançado] Modifica a escala da frequência para Codificação Posicional Rotativa para controlar a granularidade da codificação posicionai",
  "embedding.load.llama.gpuOffload/title": "Desligar GPU",
  "embedding.load.llama.gpuOffload/info": "Defina a razão de cálculo para deslocar para o GPU. Definir como off desativa a deslocação do GPU, ou auto deixe que o modelo decida.",
  "embedding.load.llama.keepModelInMemory/title": "Manter Modelo na Memória",
  "embedding.load.llama.keepModelInMemory/info": "Prevenir que o modelo seja intercambiado com a unidade de disco, garantindo acesso mais rápido ao custo de uso maior da memória RAM",
  "embedding.load.llama.tryMmap/title": "Tentar mmap()",
  "embedding.load.llama.tryMmap/info": "Carregar arquivos do modelo diretamente da unidade de disco para a memória",
  "embedding.load.seed/title": "Semente",
  "embedding.load.seed/info": "Semente Aleatória: Define a semente para geração de números aleatórios para garantir resultados reproduzíveis"
}